\subsection{Outline}
\label{sec:out}
In this paper, we consider the \KS problem in a variety of contexts. First, we provide a description of the \KS problem, and an in-depth proof of the $2k-1$ competitiveness of the Work Function Algorithm (WFA). Following this a literature review is provided, focusing on the Unifying Potential for the WFA~\cite{unifyingPotential2021}. We then have an analysis of~\cite{unifyingPotential2021}, and provide some thoughts into a potential extension of this work for caterpillar graphs. Following this, we transition to an analysis of other algorithms focusing on the bijective analysis of these algorithms, and describe a \CC environment provided for practical testing of a variety of these algorithms. We provide some experimental analysis of the algorithms within the testing suite, extending the work done in~\cite{independantStudy2023}.

\subsection{Notations and Definitions}

We provide a few definitions of notation that will be used throughout the paper for clarity.

\begin{definition}
    A \textbf{request sequence} $\sigma = \{ r_1, r_2, ... r_n\}$ can be broken up into sub-parts, where $\sigma_i$ represents the first $i$ requests from within the request sequence.
\end{definition}

\begin{definition}
    An algorithm is denoted by an abreviated name, with any generic algorithm being denoted as $ALG$.
\end{definition}

\begin{definition}
    An instance of the \KS problem consists of a metric space $M = (X,d)$, a number of servers $k >1$, an algorithm $ALG$, and a request sequence $\sigma = \{ r_1, r_2, ... r_n\}$. The \textbf{cost} inccured on this instance of the \KS problem is denoted as $ALG(\sigma)$.
\end{definition}

\subsection{Problem Description}
\label{sec:desc}
An instance of the \KS problem can be described by a metric space $M = (X, d)$, a number of servers $k>1$, and an input sequence $\sigma = (r_1, r_2, r_3, ..., r_n)$, where each $r_i$ corresponds to a point in the metric space. Each of the $k$ servers is assigned an initial starting location within the metric space (generally this is assumed to be the first $k$ requests of the input sequence). Following this, The sequence of reqeusts is processed one at a time. When a request comes in, the given algorithm $ALG$ must decide on one of the servers to service the request. It must then move said server from it's current location $x$ to the request $r_i$. This incurs a cost of $c = d(x, r_i)$. The costs are then summed across the request sequence, giving the final cost incurred by the algorithm $ALG(\Sigma)$. The goal is to have $ALG$ incur the smallest possible cost while servicing all of the requests in the sequence~\cite{OnlineComp1998}.
\\ \\
There are two major distinctions to be made between different classes of algorithms. The first is the classification of a "lazy" algorithm - one that only moves a server in order to service the current request. Non-lazy algorithms will process a request, and then potentially also move other servers preemptively in order to prepare for future requests. 

\begin{lemma}
    For any non-lazy algorithm $ALG$, there exists a lazy algorithm $LAZY$ such that $ALG(\sigma) \leq LAZY(\sigma)$.
\end{lemma}

\begin{proof}
    We can describe the algorithm $LAZY$ as follows: suppose we have our non-lazy algorithm, $ALG$. We have the algorithm $LAZY$ service requests with the same servers that $ALG$ services requests at each step. If $ALG$ were to move a server preemptively, $LAZY$ doesn't perform this move. Suppose that $ALG$ moves a server from location $x_1$ to location $x_2$ preemptively, and then later services request $r_i$ with this server. $LAZY$ will be servicing $r_i$ with the same server, except it will be moving directly from $x_1$ instead of $x_2$. By the triangle inequality, $d(x_1, r_i) \leq d(x_1, x_2) + d(x_2, r_i)$. By applying this principle throughout the request sequence, we will see that the cost of $LAZY$ will be as good if not better than that of $ALG$. By indexing the servers and ensuring that $LAZY$ services each request with the same server $ALG$ would, we have proven the lemma~\cite{OnlineComp1998}.
\end{proof}

The second major distinction is between "online" and "offline" algorithms. An offline algorithm receives the entire request sequence at once, and so as a result is able to make decisions on what server to use for the current request based off of future requests. In contrast, an online algorithm receives the request one at a time, and so is only able to make decisions based off of past requests, the current server configuration, and the current request. While online algorithms are at a severe disadvantage due to this, real world applications often rely on the performance of these algorithms. Applications range from disk access optimization, such as the two headed-disk problem~\cite{OnlineComp1998}, to police or firetruck servicing. In the context of this work, we will only consider an optimal offline algorithm $OPT$ as a comparison to the online algorithms we are analyzing.

\begin{definition}
Any optimal offline algorithm is denoted as $OPT$.
\end{definition}

\subsection{Competitive Analysis}
\label{sec:compAna}
The most popular method for analyzing the performance of online algorithms for a problem is competitive analysis. Here, we assume a "malicious adversary", who is attempting to make our algorithm $ALG$ perform as poorly as possible in relation to the performance of the optimal algorithm $OPT$. The malicious adversary is allowed to come up with any finite input, and our competitive ratio is determined by this. If for any finite input, we are able to guaruntee that $ALG$ has a cost within a certain ratio $c$ of $OPT$ (allowing for a constant additive factor), then we say that our algorithm is $c$-competitive~\cite{OnlineComp1998}. So, if we define $ALG(\sigma)$ as the cost $ALG$ incurs while processing request $\sigma$, then we have the following definition: 

\begin{definition}
\label{def:comp}
Algorithm $ALG$ is said to be \textbf{\textit{c}-competitive} if for every finite request sequence \s, $ALG(\sigma) \leq c\cdot OPT(\sigma)+\alpha$ for some constant $\alpha$.
\end{definition}

Competitive analysis is in some sense similar to "worst case" analysis, where we try to see how poorly an algorithm will ever perform. It is worth noting that some algorithms such as a greedy algorithm may perform very well on the majority of inputs, but given specific inputs may not be competitive at all. That is, given some value $c$, a finite length input can be found such that the algorithm doesn't satisfy the above definition~\cite{OnlineComp1998}.

\subsection{Literature Review}

The \KS problem originally arose as an abstraction of the paging problem, where the goal is to minimize the number of page faults in a cache as new memory access requests come in. It was originally proposed by Manasse, McGeoch, and Sleator~\cite{KS1988}, where they also showed a 2 competitive 2 server algorithm for any metric space. They additionally demonstrated a $k$ competitive algorithm for metric spaces with $k+1$ points. Two years later they wrote a paper showing generally competitive algorithms and some applications~\cite{KS1990}. Manasse, McGeoch and Sleator additionally proposed the $k$ server conjecture: that is, that there exists a $k$ competitive algorithm for any metric space with $k$ servers. 

Following this, various algorithms were proposed with improving competitive ratios~\cite{MP1990, harm2000}. Koutsoupias and Papadimitriou~\cite{KS1990} showed that the Work Function Algorithm is $2k-1$ competitive in 1994, which to-date remains the best competitive ratio for the \KS problem for generic metric spaces.

Various other algorithms have been constructed for special metric spaces, most notably including the double coverage algorithm for line metric spaces~\cite{new1991}. This algorithm was later generalized to a $k$ competitive algorithm for trees~\cite{tree1991}.

The work function algorithm remains the strongest candidate for a $k$ competitive algorithm for the \KS problem. Various improvements have been made, includinga proof that the work function algorithm is $k$ competitive on various classes of the problem. These include line metric spaces, metric spaces with $k+1$ points, metric spaces with $k+2$ points, multiray spaces (where there is a single "center" point, with rays extending out from it, i.e. only one node is connected to more than 2 other nodes), on tree graphs for $k=3$~\cite{unifyingPotential2021}, and on the Manhattan Plane for $k=3$~\cite{MP2002}. 

Most recently, in 2021 a paper by Coester and Koutsoupias proposes a new potential function for proving the competitiveness of the work function algorithm~\cite{unifyingPotential2021}. This paper proposes a \textit{unifying potential} that can be used to prove various subcases of $k$ competitiveness for the \KS problem. This shows promise as previous proofs relied on a variety of different potential functions, that were often non-generalizable to other cases.
%other forms of analysis / other results.
Additionally, other forms of analysis and modifications to the \KS problem have been proposed. Analysis methods include Direct analysis where the cost of the algorithm is directly compared to the cost of the optimal algorithm on a given request sequence, and Max-Max ratios where the maximum cost of an algorithm on a set of inputs is compared to the maximum cost of another on the same set~\cite{MAXMAX2005}. Additionally, Bijective Analysis and its generalization (Stochatic Dominance) have been proposed as methods for comparing algorithms that favor algorithms that are able to consistently outperform other algorithms on a given set of inputs~\cite{bij2016}. 

Modifications to the \KS problem primarily include randomized algorithms, where the algorithm is allowed to make random decisions in order to improve its average case performance~\cite{OnlineComp1998}. Additional modifications include adding advice or lookahead for the algorithm. Great improvemenets can be made in an algorithms competitive ratio if it is able to get some sort of information, either about the current configuration of $OPT$, or about where the next few requests will be~\cite{advice2015}.

\subsection{$k$ competitive lower bound}
\label{sec:lowerBound}

In this section, we follow a proof for a lower bound of the $k$ server problem, as described in~\cite{server2009}. Additional modifications include the \KS problem with lookahead~\cite{}

\begin{lemma}
    For a metric space $M$ where $|M| > k$, no online algorithm $ALG$ can have competitive ratio less than $k$
\end{lemma}

\begin{proof}
    We begin by restricting all requests to a sub-metric space which has $k+1$ points. Within this metric space, there are $k+1$ possible configurations of the $k$ servers. We begin by maintaining $k$ offline algorithms (denoted $OPT_1, OPT_2, ..., OPT_k$), where each of these algorithms has a different configuration, and none have the same configuration as $ALG$. We denote the configuration of $ALG$ as $x_1, x_2, ...x_k \in M$. We begin by requesting the point $r \not \in x_1, x_2, ...x_k$. All of the offline algorithms are able to service this request without moving any servers, while $ALG$ must move some server from $x_i$ to service this point. At this point we will have $OPT_i$, the offline algorithm that has no server at $x_i$, move its server from $r$ to $x_i$ preemptively. We then repeat this process.

    This shows that the cost incurred by $ALG$ will be equal to the sum of the costs incurred by all of the offline algorithms, showing that the competitive ratio of $ALG$ will be at least $k$.
\end{proof}